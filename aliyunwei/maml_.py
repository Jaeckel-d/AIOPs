import json
import os

import numpy as np
import torch
from torch import nn
from tqdm import tqdm
import datetime as dt
from utils import move_to_device
from copy import deepcopy
import torch.nn.functional as F


class MAML:
    def __init__(self,
                 model,
                 num_inner_steps,
                 inner_lr,
                 outer_lr,
                 first_order,
                 epochs=10,
                 runtime_path="./MAML/",
                 device="cuda:0",
                 finetune=False, ):
        """Initializes the MAML.

        Args:
            model (Model): Meta-learner.
            num_inner_steps (int): Number of inner-loop optimization steps
                during adaptation.
            inner_lr (float): Learning rate for inner-loop optimization.
            outer_lr (float): Learning rate for outer-loop optimization.
            first_order (bool): Whether to use first-order approximation
                when computing gradient for meta-update.
        """
        self.device = device
        self.model = model.to(self.device)

        self.model_params = list(model.parameters())
        self.num_inner_steps = num_inner_steps
        self.inner_lr = inner_lr
        self.outer_lr = outer_lr
        self.outer_optimizer = torch.optim.Adam(self.model_params,
                                                self.outer_lr)
        self.runtime_path = runtime_path
        self.model_weight_name = "10_shot_{}_ckpt.pt".format(
            dt.datetime.strftime(dt.datetime.now(), '%Y-%m-%d-%H_%M_%S'))

        if first_order:
            self.model_weight_name = "fomaml_" + self.model_weight_name
        self.model_weight_path = self.runtime_path + self.model_weight_name

        # self.loss_function = nn.MSELoss()
        self.loss_function = nn.BCEWithLogitsLoss()
        self.epochs = epochs
        self.first_order = first_order

        self.finetune = finetune

    def inner_loop(self, x_support, y_support):
        """Computes the adapted model parameters on a given support set.

        Args:
            x_support (Tensor): Support set inputs.
            y_support (Tensor): Support set outputs.

        Returns:
            tuple[Tensor]: Adapted model parameters. Length `num_params`.
            list[float]: Support set loss throughout inner loop.
                Length `num_inner_steps` + 1.
            float: Frobenius norm of initial adaptation gradient.
        """
        support_losses = []

        # Clone outer parameters for adaptation
        adapt_params = [w.clone() for w in self.model_params]
        # Perform `num_inner_steps` iterations of SGD
        for inner_step in range(self.num_inner_steps + 1):
            # Compute support loss and adaptation gradient
            support_loss = torch.tensor(.0).to(self.device)
            for i, src in enumerate(x_support):
                src = move_to_device(src, self.device)
                tgt = y_support[:, i]
                tgt = torch.unsqueeze(tgt.float().to(self.device), dim=1)
                pred_y_support = self.model.forward(src, adapt_params)
                support_loss += self.loss_function(pred_y_support, tgt)
            support_losses.append(support_loss.item())
            adapt_grad = torch.autograd.grad(support_loss, adapt_params,
                                             create_graph=not self.first_order)
            if inner_step == 0:
                init_grad = torch.concat([g.flatten() for g in adapt_grad])
                init_grad_norm = torch.linalg.norm(init_grad).item()
            if inner_step == self.num_inner_steps:
                break
            # Update adaptation parameters
            adapt_params = [w - self.inner_lr * g
                            for w, g in zip(adapt_params, adapt_grad)]

        return adapt_params, support_losses, init_grad_norm

    def inner_loop_plot(self, x_support, y_support, x, inner_steps_plot):
        """Adapts the model on a given support set and periodically
        generates predictions for `x`.

        Args:
            x_support (Tensor): Support set inputs.
            y_support (Tensor): Support set outputs.
            x (Tensor): Prediction inputs.
            inner_steps_plot (list[int]): Inner step(s) at which
                predictions for `x` are generated by the adapted model.
                For example, if `inner_steps_plot` = [1, 2, 10], then
                10 inner-optimization steps are performed, and
                predictions are generated after 0, 1, 2, and 10 steps.

        Returns:
            dict[int, ndarray]: Key is inner optimization step.
                Value is prediction output.
        """
        pred_y = {}
        # Clone outer parameters for adaptation
        adapt_params = [w.clone() for w in self.model_params]
        # Perform `max(inner_steps_plot)` iterations of SGD
        for inner_step in range(max(inner_steps_plot) + 1):
            # Compute support loss and adaptation gradient
            pred_y_support = self.model.forward(x_support, adapt_params)
            support_loss = self.loss_function(pred_y_support, y_support)
            if inner_step == 0 or inner_step in inner_steps_plot:
                pred_y[inner_step] = (
                    self.model.forward(x, adapt_params)
                ).detach().cpu().numpy()
            adapt_grad = torch.autograd.grad(support_loss, adapt_params,
                                             create_graph=not self.first_order)
            # Update adaptation parameters
            adapt_params = [w - self.inner_lr * g
                            for w, g in zip(adapt_params, adapt_grad)]
        return pred_y

    def outer_step(self, task_batch):
        """Computes the MAML loss (i.e., mean query loss) on a batch of tasks.

        Args:
            task_batch (list): Batch of tasks. Length `batch_size`.

        Returns:
            Tensor: Query loss averaged over batch. Scalar.
            ndarray: Support set loss throughout inner loop, averaged
                over batch. Shape (`num_inner_steps` + 1,).
            float: Frobenius norm of initial adaptation gradient,
                averaged over batch.
        """
        query_loss_batch = []
        support_losses_batch = []
        init_grad_norm_batch = []

        task_num = task_batch[-1].shape[0]
        for i in range(task_num):
            x_support, x_query, y_support, y_query = task_batch[0], task_batch[1], task_batch[2], task_batch[3]
            # Obtain adapted parameters from inner loop
            adapt_params, support_losses, init_grad_norm = \
                self.inner_loop(x_support, y_support)
            support_losses_batch.append(support_losses)

            # Compute query loss using adapted parameters

            query_loss = torch.tensor(.0).to(self.device)
            for j, src in enumerate(x_query):
                src = move_to_device(src, self.device)
                tgt = y_query[:, j]
                tgt = torch.unsqueeze(tgt.float().to(self.device), dim=1)
                pred = self.model.forward(src, adapt_params)
                query_loss += self.loss_function(pred, tgt)

            query_loss_batch.append(query_loss)

        mean_query_loss = torch.stack(query_loss_batch).mean()
        mean_support_losses = np.mean(np.array(support_losses_batch), axis=0)
        mean_init_grad_norm = np.mean(np.array(init_grad_norm_batch))
        return mean_query_loss, mean_support_losses, mean_init_grad_norm

    def train(self, data_train, log_interval, filename="./logs/train.json"):
        """Trains the MAML.

        Args:
            data_train (list[list[list[Tensor]]]): Training dataset.
                First dimension has length `num_train_steps//batch_size`.
                Second dimension has length `batch_size`.
                Innermost dimension has length 4 and consists of
                support set inputs, support set outputs, query set inputs,
                and support set outputs, respectively.
            log_interval (int): Frequency of metric logging.
            filename (str): File to which metrics are logged.

        Returns:
            list[float]: Query loss (averaged over batch) throughout
                training loop. Length `num_train_steps`.
            ndarray: Frobenius norm of initial adaptation gradient
                (averaged over batch) throughout training loop.
                Shape (`num_train_steps`,).
        """
        log = {}
        query_losses = []
        init_grad_norms = []
        for epoch in range(self.epochs):
            print(f"------------------Epoch:{epoch + 1}---------------------")
            for outer_step, task_batch in enumerate(tqdm(data_train)):
                mean_query_loss, mean_support_losses, mean_init_grad_norm = \
                    self.outer_step(task_batch)
                self.outer_optimizer.zero_grad()
                mean_query_loss.backward()
                self.outer_optimizer.step()
                # Log metrics
                if outer_step % log_interval == 0:
                    print(f"Iteration {outer_step}: ")
                    print("MAML loss (query set loss, batch average): ",
                          f"{mean_query_loss.item():.3f}")
                    print("Pre-adaptation support set loss (batch average): ",
                          f"{mean_support_losses[0]:.3f}")
                    print("Post-adaptation support set loss (batch average): ",
                          f"{mean_support_losses[-1]:.3f}")
                    print("Norm of initial adaptation gradient (batch average): ",
                          f"{mean_init_grad_norm:.3f}")
                    print("-" * 50)
                    log[outer_step] = {
                        "Query set loss": mean_query_loss.item(),
                        "Pre-adaptation support set loss": mean_support_losses[0],
                        "Post-adaptation support set loss": mean_support_losses[-1],
                        "Norm of initial adaptation gradient": mean_init_grad_norm,
                    }
                    query_losses.append(mean_query_loss.item())
                    init_grad_norms.append(mean_init_grad_norm)
        # Save metrics
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        with open(filename, "w") as f:
            json.dump(log, f)

        self.save_model(self.model_weight_path)

        return query_losses, init_grad_norms

    def test(self, data_test, num_inner_steps=None):
        """Evaluates the MAML on test tasks.

        Args:
            data_test (list[list[list[Tensor]]]): Test dataset.
                See description for `data_train` in train().
            num_inner_steps (int): Number of inner-loop optimization steps
                during adaptation. If not provided, then training-time
                value is used.

        Returns:
            float: Query loss averaged over test tasks. Scalar.
            float: Standard deviation of query loss. Scalar.
            ndarray: Support set loss throughout inner loop,
                averaged over test tasks. Size (`num_inner_steps + 1`,).
            ndarray: Standard deviation of support set loss throughout
                inner loop, averaged over test tasks.
                Size (`num_inner_steps` + 1, 1).
        """
        query_losses = []
        support_losses = []
        num_test_tasks = len(data_test) * len(data_test[0])

        # Set `num_inner_steps` according to argument value
        if num_inner_steps is not None:
            prev_num_inner_steps = self.num_inner_steps
            self.num_inner_steps = num_inner_steps
        # Evaluate test tasks
        for task_batch in data_test:
            q_loss, s_losses, _ = self.outer_step(task_batch)
            query_losses.append(q_loss.item())
            support_losses.append(s_losses)
        # Reset original value of `num_inner_steps`
        if num_inner_steps is not None:
            self.num_inner_steps = prev_num_inner_steps
        # Compute statistics
        mean_query_loss = np.mean(query_losses)
        CI_95_query_loss = 1.96 * np.std(query_losses) / np.sqrt(num_test_tasks)
        np_support_losses = np.array(support_losses)
        mean_support_losses = np.mean(np_support_losses, axis=0)
        CI_95_support_losses = (1.96 * np.std(np_support_losses, axis=0)
                                / np.sqrt(num_test_tasks))
        print(f"Evaluation statistics on {num_test_tasks} test tasks: ")
        print("MAML loss:")
        print(f"Mean: {mean_query_loss:.3f}")
        print(f"95% confidence interval: {CI_95_query_loss:.3f}")
        return (mean_query_loss, CI_95_query_loss,
                mean_support_losses, CI_95_support_losses)

    def finetune_(self, finetune_dataset, num_inner_steps=100, load_path=None, save_path=None):
        fast_weight = None
        threshold = 0.5

        if load_path:
            self.load_model(load_path)
        for i, task_batch in enumerate(tqdm(finetune_dataset)):
            x_support, x_query, y_support, y_query = task_batch[0], task_batch[1], task_batch[2], task_batch[3]

            corrects = [0 for _ in range(num_inner_steps + 1)]
            model_params = list(self.model.parameters())
            adapt_params = [w.clone() for w in model_params]

            # 1. run the i-th task and compute loss for k=0
            support_loss = torch.tensor(.0).to(self.device)
            for j, src in enumerate(x_support):
                src = move_to_device(src, self.device)
                tgt = y_support[:, j]
                tgt = torch.unsqueeze(tgt.float().to(self.device), dim=1)
                pred = self.model.forward(src, adapt_params)
                support_loss += self.loss_function(pred, tgt)

            adapt_grad = torch.autograd.grad(support_loss, adapt_params,
                                             create_graph=not self.first_order)
            fast_weight = [w - self.inner_lr * g
                           for w, g in zip(adapt_params, adapt_grad)]

            # this is the loss and accuracy before first update
            with torch.no_grad():
                pred_query = []
                labels = []
                for j, src in enumerate(x_query):
                    src = move_to_device(src, self.device)
                    tgt = y_query[:, j]
                    tgt = torch.unsqueeze(tgt.float().to(self.device), dim=1)
                    pred = self.model.forward(src, adapt_params)
                    # pred_q = F.softmax(pred, dim=1).argmax(dim=1)
                    pred_q = (pred >= threshold).float()
                    pred_query.append(pred_q)
                    labels.append(tgt)
                    # scalar
                correct = (torch.eq(torch.tensor(pred_query), torch.tensor(labels)).sum().item()) / len(labels)
                corrects[0] = corrects[0] + correct

            with torch.no_grad():
                pred_query = []
                labels = []
                for j, src in enumerate(x_query):
                    src = move_to_device(src, self.device)
                    tgt = y_query[:, j]
                    tgt = torch.unsqueeze(tgt.float().to(self.device), dim=1)
                    pred = self.model.forward(src, fast_weight)
                    # pred_q = F.softmax(pred, dim=1).argmax(dim=1)
                    pred_q = (pred >= threshold).float()
                    pred_query.append(pred_q)
                    labels.append(tgt)
                    # scalar
                correct = (torch.eq(torch.tensor(pred_query), torch.tensor(labels)).sum().item()) / len(labels)
                corrects[1] = corrects[1] + correct

            for k in range(1, num_inner_steps):

                support_loss = torch.tensor(.0).to(self.device)
                for j, src in enumerate(x_support):
                    src = move_to_device(src, self.device)
                    tgt = y_support[:, j]
                    tgt = torch.unsqueeze(tgt.float().to(self.device), dim=1)
                    pred = self.model.forward(src, fast_weight)
                    support_loss += self.loss_function(pred, tgt)

                adapt_grad = torch.autograd.grad(support_loss, fast_weight,
                                                 create_graph=not self.first_order)
                fast_weight = [w - self.inner_lr * g
                               for w, g in zip(fast_weight, adapt_grad)]

                with torch.no_grad():
                    pred_query = []
                    labels = []
                    for j, src in enumerate(x_query):
                        src = move_to_device(src, self.device)
                        tgt = y_query[:, j]
                        tgt = torch.unsqueeze(tgt.float().to(self.device), dim=1)
                        pred = self.model.forward(src, fast_weight)
                        # pred_q = F.softmax(pred, dim=1).argmax(dim=1)
                        pred_q = (pred >= threshold).float()
                        pred_query.append(pred_q)
                        labels.append(tgt)
                        # scalar
                    correct = (torch.eq(torch.tensor(pred_query), torch.tensor(labels)).sum().item()) / len(labels)
                    corrects[k + 1] = corrects[k + 1] + correct
            print(corrects)
            # print(pred_query)
            # print(corrects)

        if save_path:
            # self.save_model(save_path)
            self.save_model_params(save_path, fast_weight)

    def save_model(self, path):
        torch.save(self.model, path)

    def save_model_params(self, path, params):
        model = deepcopy(self.model)
        state_dict = model.state_dict()
        for i, (name, param) in enumerate(state_dict.items()):
            state_dict[name] = params[i]
        model.load_state_dict(state_dict)
        torch.save(model, path)

    def load_model(self, path):
        self.model = torch.load(path)
